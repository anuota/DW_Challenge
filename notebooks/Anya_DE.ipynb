{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeutscheWelle Data Engineering Challenge\n",
    "\n",
    "9.05.2023\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect via Gmail API\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Script that dowloads emails from gmail inbox\n",
    "\n",
    "## from https://developers.google.com/gmail/api/reference/rest/v1/users.messages.attachments/get \n",
    "# # Method to download email attachments from gmail api\n",
    "##\n",
    "# userId dwdechallenge@gmail.com / me (authenticated user)\n",
    "# messageId \n",
    "# id  2022-09-15-23-04-08-EDT-Historical-Report-ENTRFacebook-2022-06-16--2022-09-16.csv \n",
    "#GET https://gmail.googleapis.com/gmail/v1/users/{userId}/messages/{messageId}/attachments/{id}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googleapiclient.discovery import build\n",
    "from google.oauth2.credentials import Credentials\n",
    "from googleapiclient.errors import HttpError\n",
    "import base64\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 17] File exists: './Downloaded_files'\n",
      "2022-09-15-23-04-08-EDT-Historical-Report-ENTRFacebook-2022-06-16--2022-09-16.csv\n",
      "File 2022-09-15-23-04-08-EDT-Historical-Report-ENTRFacebook-2022-06-16--2022-09-16.csv is saved at ./Downloaded_files\n",
      "2022-09-16-23-04-01-EDT-Historical-Report-ENTRFacebook-2022-06-17--2022-09-17.csv\n",
      "File 2022-09-16-23-04-01-EDT-Historical-Report-ENTRFacebook-2022-06-17--2022-09-17.csv is saved at ./Downloaded_files\n",
      "2022-09-17-23-04-42-EDT-Historical-Report-ENTRFacebook-2022-06-18--2022-09-18.csv\n",
      "File 2022-09-17-23-04-42-EDT-Historical-Report-ENTRFacebook-2022-06-18--2022-09-18.csv is saved at ./Downloaded_files\n",
      "2022-09-18-23-03-32-EDT-Historical-Report-ENTRFacebook-2022-06-19--2022-09-19.csv\n",
      "File 2022-09-18-23-03-32-EDT-Historical-Report-ENTRFacebook-2022-06-19--2022-09-19.csv is saved at ./Downloaded_files\n",
      "2022-09-20-23-03-35-EDT-Historical-Report-ENTRFacebook-2022-06-21--2022-09-21.csv\n",
      "File 2022-09-20-23-03-35-EDT-Historical-Report-ENTRFacebook-2022-06-21--2022-09-21.csv is saved at ./Downloaded_files\n",
      "2022-09-21-23-04-26-EDT-Historical-Report-ENTRFacebook-2022-06-22--2022-09-22.csv\n",
      "File 2022-09-21-23-04-26-EDT-Historical-Report-ENTRFacebook-2022-06-22--2022-09-22.csv is saved at ./Downloaded_files\n",
      "2022-09-23-23-04-15-EDT-Historical-Report-ENTRFacebook-2022-06-24--2022-09-24.csv\n",
      "File 2022-09-23-23-04-15-EDT-Historical-Report-ENTRFacebook-2022-06-24--2022-09-24.csv is saved at ./Downloaded_files\n"
     ]
    }
   ],
   "source": [
    "#works! \n",
    "## 1) find email for the certain query i.e. 'reports' in emails subject/body\n",
    "## 2) get message ids\n",
    "## 3) get attachments ids\n",
    "## 4) downloads attachments for every found email\n",
    "\n",
    "def search_email(service, query_string,label_ids=[]):\n",
    "    #try:\n",
    "        message_list_response = service.users().messages().list(\n",
    "            userId='me',\n",
    "            labelIds=label_ids,\n",
    "            q=query_string\n",
    "        ).execute()\n",
    "\n",
    "        message_items=message_list_response.get('messages')\n",
    "        nextPageToken=message_list_response.get('nextPageToken')\n",
    "\n",
    "        while nextPageToken:\n",
    "            message_list_response = service.users().messages().list(\n",
    "                    userId='me',\n",
    "                    labelIds=label_ids,\n",
    "                    q=query_string,\n",
    "                    pageToken=nextPageToken\n",
    "            ).execute()    \n",
    "\n",
    "            message_items.extend(message_list_response.get('messages'))\n",
    "            nextPageToken=message_items.get('nextPageToken')\n",
    "        return message_items\n",
    "    #except Exception as e:\n",
    "    #    return 'No emails found'\n",
    "\n",
    "def get_message_detail(msg_id,msg_format='metadata',metadata_headers: list=None):\n",
    "    message_detail=service.users().messages().get(\n",
    "        userId='me',\n",
    "        id=msg_id,\n",
    "        format=msg_format,\n",
    "        metadataHeaders=metadata_headers\n",
    "    ).execute()\n",
    "    return message_detail\n",
    "\n",
    "def get_file_data(message_id,attachment_id,file_name,save_location):\n",
    "    response=service.users().messages().attachments().get(\n",
    "        userId='me',\n",
    "        messageId=message_id,\n",
    "        id=attachment_id\n",
    "    ).execute()\n",
    "\n",
    "    file_data=base64.urlsafe_b64decode(response.get('data').encode('UTF-8'))\n",
    "    return file_data\n",
    "    \n",
    "#from google.auth.transport.requests import Request\n",
    "#creds.refresh(Request())\n",
    "\n",
    "\n",
    "# If modifying these scopes, delete the file token.json.\n",
    "SCOPES = ['https://www.googleapis.com/auth/gmail.readonly']\n",
    "\n",
    "creds = Credentials.from_authorized_user_file('../token.json', SCOPES)\n",
    "service = build('gmail', 'v1', credentials=creds)\n",
    "\n",
    "\n",
    "## search for emails with 'report' in the subject line\n",
    "\n",
    "#query_string='has:attachment' # all email with attachments\n",
    "query_string='filename:report*csv' #find all the email with attachment in csv format containing 'report' in their name\n",
    "#query_string='report' # all email with report in subject/body text\n",
    "email_messages = search_email(service, query_string,['INBOX'])\n",
    "#print(email_messages)\n",
    "try:\n",
    "    path = os.path.join('./', 'Downloaded_files')\n",
    "    os.mkdir(path)\n",
    "except OSError as error:\n",
    "    print(error) \n",
    "save_location='./Downloaded_files'\n",
    "## download attachments for found emails \n",
    "\n",
    "for email_message in email_messages:\n",
    "    #print(email_message['id'])\n",
    "    #187ffb2fbf3c1bc5\n",
    "    messageDetail = get_message_detail(email_message['id'],msg_format='full',metadata_headers=['parts'])\n",
    "    #print(messageDetail)\n",
    "    messageDetailPayload = messageDetail.get('payload')\n",
    "\n",
    "    if 'parts' in messageDetailPayload:\n",
    "        for msgPayload in messageDetailPayload['parts']:\n",
    "            file_name = msgPayload['filename']\n",
    "            body=msgPayload['body']\n",
    "\n",
    "            if 'attachmentId' in body:\n",
    "                attachment_id = body['attachmentId']\n",
    "                attachment_content= get_file_data(email_message['id'],attachment_id,file_name,save_location)\n",
    "                print(file_name)\n",
    "                \n",
    "                #with open(os.path.join(save_location + '/' 'Downloaded_files' + '/' + file_name),'wb') as _f:\n",
    "                with open(os.path.join(save_location + '/' + file_name),'wb') as _f:\n",
    "                    _f.write(attachment_content)\n",
    "                    print(f'File {file_name} is saved at {save_location}')\n",
    "    time.sleep(0.5)\n",
    "    #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import psycopg2\n",
    "import os\n",
    "import glob\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write a python script that identifies the correct daily reports\n",
    "\n",
    "!ls /Users/anya/SCIENCE/DW_Challenge/Junior_DE_Task/daily_reports/\n",
    "\n",
    "#daily_repo_filepath= \"/Users/anya/SCIENCE/DW_Challenge/Junior_DE_Task/daily_reports/\"\n",
    "daily_repo_filepath='./Downloaded_files' # downloaded files via API\n",
    "#file1='2022-09-15-23-04-08-EDT-Historical-Report-ENTRFacebook-2022-06-16--2022-09-16.csv'\n",
    "files=glob.glob(os.path.join(daily_repo_filepath,\"*.csv\"))\n",
    "##date\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(daily_repo_filepath+file1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to postgres database\n",
    "param_dic = {\n",
    "    \"host\"      : \"localhost\",\n",
    "    \"database\"  : \"postgres\",\n",
    "    \"user\"      : \"postgres\",\n",
    "}\n",
    "def connect(params_dic):\n",
    "    \"\"\" Connect to the PostgreSQL database server \"\"\"\n",
    "    conn = None\n",
    "    try:\n",
    "        # connect to the PostgreSQL server\n",
    "        print('Connecting to the PostgreSQL database...')\n",
    "        conn = psycopg2.connect(**params_dic)\n",
    "    except (Exception, psycopg2.DatabaseError) as error:\n",
    "        print(error)\n",
    "        sys.exit(1) \n",
    "    print(\"Connection successful\")\n",
    "    return conn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = connect(param_dic)\n",
    "\n",
    "def execute_query(conn, query):\n",
    "    \"\"\" Execute a single query \"\"\"\n",
    "    connect(param_dic)\n",
    "    ret = 0 # Return value\n",
    "    cursor = conn.cursor()\n",
    "    try:\n",
    "        cursor.execute(query)\n",
    "        conn.commit()\n",
    "    except (Exception, psycopg2.DatabaseError) as error:\n",
    "        print(\"Error: %s\" % error)\n",
    "        conn.rollback()\n",
    "        cursor.close()\n",
    "        return 1\n",
    "\n",
    "    # If this was a select query, return the result\n",
    "    if 'select' in query.lower():\n",
    "        ret = cursor.fetchall()\n",
    "    cursor.close()\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create postgres database\n",
    "\n",
    "cols=','.join([column.replace(\" \", \"_\").replace(\"?\",\"\").lower() + str(' VARCHAR') for column in data.columns.values])\n",
    "new_table='new_table'\n",
    "\n",
    "#query=(\"DROP TABLE IF EXISTS %s;\"%(new_table))\n",
    "query=(\"CREATE TABLE IF NOT EXISTS %s(%s)\" %(new_table,(cols))) \n",
    "\n",
    "execute_query(conn, query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2nd option create postgres database\n",
    "def create_postgres (db,new_table):\n",
    "    cols=cols=','.join([column.replace(\" \", \"_\").replace(\"?\",\"\").lower() + str(' VARCHAR') for column in data.columns.values])\n",
    "\n",
    "    conn = connect(param_dic)\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    #drop table if already exists\n",
    "    cur.execute(\"DROP TABLE IF EXISTS %s;\"%(new_table))\n",
    "    #create database\n",
    "    cur.execute(\"CREATE TABLE IF NOT EXISTS %s(%s)\" %(new_table,(cols))) \n",
    "    conn.commit()\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## data columns to create SQL database \n",
    "#VARCHAR for csv input\n",
    "cols=','.join([column.replace(\" \", \"_\").replace(\"?\",\"\").lower() + str(' VARCHAR') for column in data.columns.values])\n",
    "cols\n",
    "# for column in data.columns.values:\n",
    "#     print(column.replace(\" \", \"_\") + str(type(data[column][0])).replace(\"class\", \"\").replace(\"<\", \"\").replace(\">\", \"\").replace(\"'\", \"\").replace(\"numpy.int64\",\"int\").replace(\"numpy.float64\",\"float\").replace(\"str\",\"text\") +',')    \n",
    "\n",
    "#same with list comprehension\n",
    "# types=([ column.replace(\" \", \"_\") + str(type(data[column][0])).replace(\"class\", \"\").replace(\"<\", \"\").replace(\">\", \"\").replace(\"'\", \"\").replace(\"numpy.int64\",\"int\").replace(\"numpy.float64\",\"float\").replace(\"str\",\"text\") for column in data.columns.values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## old create database\n",
    "#connect database\n",
    "conn = psycopg2.connect(\"host=localhost dbname=postgres user=postgres\")\n",
    "cur = conn.cursor()\n",
    "\n",
    "#create database\n",
    "cur.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS reports_varchar(\n",
    "    page_name VARCHAR,\n",
    "    user_name VARCHAR,\n",
    "    facebook_id VARCHAR,\n",
    "    page_category VARCHAR,\n",
    "    page_admin_top_country VARCHAR,\n",
    "    page_description VARCHAR,\n",
    "    page_created VARCHAR,\n",
    "    likes_at_posting VARCHAR,\n",
    "    followers_at_posting VARCHAR,\n",
    "    post_created VARCHAR,\n",
    "    post_created_date VARCHAR,\n",
    "    post_created_time VARCHAR,\n",
    "    type VARCHAR,\n",
    "    total_interactions VARCHAR,\n",
    "    likes VARCHAR,\n",
    "    comments VARCHAR,\n",
    "    shares VARCHAR,\n",
    "    love VARCHAR,\n",
    "    wow VARCHAR,\n",
    "    haha VARCHAR,\n",
    "    sad VARCHAR,\n",
    "    angry VARCHAR,\n",
    "    care VARCHAR,\n",
    "    video_share_status VARCHAR,\n",
    "    is_video_owner VARCHAR,\n",
    "    post_views VARCHAR,\n",
    "    total_views VARCHAR,\n",
    "    total_views_for_all_crossposts VARCHAR,\n",
    "    video_length VARCHAR,\n",
    "    url VARCHAR,\n",
    "    message VARCHAR,\n",
    "    link VARCHAR,\n",
    "    final_link VARCHAR,\n",
    "    image_VARCHAR VARCHAR,\n",
    "    link_VARCHAR VARCHAR,\n",
    "    description VARCHAR,\n",
    "    sponsor_id VARCHAR,\n",
    "    sponsor_name VARCHAR,\n",
    "    sponsor_category VARCHAR,\n",
    "    overperforming_score_ VARCHAR\n",
    ")\n",
    "\"\"\")\n",
    "conn.commit()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Upload data to the postgres database\n",
    "\n",
    "def upload_csv_to_postgres(table, csv):\n",
    "    conn = connect(param_dic)\n",
    "    cur = conn.cursor()\n",
    "    try:\n",
    "        [cur.copy_expert(\"COPY %s FROM STDIN WITH DELIMITER ',' CSV \"%(table), open(csv_file)) for csv_file in files]\n",
    "    except (Exception, psycopg2.DatabaseError) as error:\n",
    "        print(\"Error: %s\" % error)\n",
    "        conn.rollback()\n",
    "        cur.close()\n",
    "        return 1\n",
    "\n",
    "    conn.commit()\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table='new_table'\n",
    "\n",
    "create_postgres(data,table)\n",
    "upload_csv_to_postgres(table,files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = connect(param_dic)\n",
    "execute_query(conn, 'SELECT Page_Name User_Name FROM new_table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##SQL script to process the data and push the data to GIT\n",
    "## create tables \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Have a look at the dwh_dl_facebook_post_insights.csv. You will find there 3 rows for seperate posts. Import the csv to a database table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_insights='/Users/anya/SCIENCE/DW_Challenge/Junior_DE_Task/'\n",
    "db_insights=pd.read_csv(path_insights+'dwh_dl_facebook_post_insights.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "table='insights_draft'\n",
    "data=db_insights\n",
    "files=glob.glob(os.path.join(path_insights,\"*.csv\"))\n",
    "#conn = connect(param_dic)\n",
    "create_postgres(data,table)\n",
    "upload_csv_to_postgres(table,files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Please write a SQL script to unpack the json in column post_video_view_time_by_age_bucket_and_gender in a new table, while preserving the other columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SELECT \n",
    "   id, \n",
    "   data::json->'name' as name,\n",
    "   data::json->'author' ->> 'last_name' as author\n",
    "FROM books;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = connect(param_dic)\n",
    "\n",
    "sel_column='post_video_view_time_by_age_bucket_and_gender'\n",
    "table='insights_draft'\n",
    "execute_query(conn,'SELECT %s FROM %s'%(sel_column,table))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execute_query(conn,'SELECT %s FROM %s'%(sel_column,table))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execute_query(conn, \"SELECT %s  FROM %s, data::jsons->'U.18-24' as U.18-24, data::jsons->'U.25-34' as U.25-34\"%(sel_column,table))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execute_query(conn,'SELECT %s FROM %s, json_each(%s)'%(sel_column,table,sel_column))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sel_column)\n",
    "conn = connect(param_dic)\n",
    "execute_query(conn, \"SELECT %s json_data -> '%s' %s, FROM %s;\" %(sel_column,sel_column,'foo',table))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = psycopg2.connect(\"host=localhost dbname=postgres user=postgres\")\n",
    "cur = conn.cursor()\n",
    "cur.execute(\"\"\"\n",
    "    SELECT json_data \n",
    "        post_video_view_time_by_age_bucket_and_gender -> \"U.18-24\" as U1\n",
    "    FROM insights_draft;\n",
    "    \"\"\")\n",
    "conn.commit()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = psycopg2.connect(\"host=localhost dbname=postgres user=postgres\")\n",
    "cur = conn.cursor()\n",
    "cur.execute(\"\"\"\n",
    "    SELECT (post_video_view_time_by_age_bucket_and_gender) from insights_draft;\n",
    "    \"\"\")\n",
    "conn.commit()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execute_query(conn,'select jsonb_object_keys(%s) from %s;'%(sel_column,table))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sel_column)\n",
    "conn = connect(param_dic)\n",
    "execute_query(conn, \"SELECT %s -> 'U.18-24' AS U1 FROM %s\" %(sel_column,table))\n",
    "\n",
    "#SELECT post_video_view_time_by_age_bucket_and_gender FROM insights_draft;\n",
    "\n",
    "#SELECT post_video_view_time_by_age_bucket_and_gender json_data ->> 'U.18-24' AS U1 FROM insights_draft; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execute_query(conn,\"select \n",
    "    json_data #> \n",
    "       '{pizzas,1,additionalToppings}'  \n",
    "        as additional_toppings_2nd_pizza\n",
    "from test;\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write a script for the sums per post for each gender. Additionally create the sum for age 18-34 of all genders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SELECT SUM(Quantity) FROM OrderDetails;\n",
    "# add where statement WHERE Country='Mexico' WHERE Country='Mexico'\n",
    "# SELECT * FROM Products WHERE Price BETWEEN 50 AND 60;"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvDE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
