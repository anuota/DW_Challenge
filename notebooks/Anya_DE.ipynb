{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeutscheWelle Data Engineering Challenge\n",
    "\n",
    "9.05.2023\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect via Gmail API\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Script that dowloads emails from gmail inbox\n",
    "\n",
    "## from https://developers.google.com/gmail/api/reference/rest/v1/users.messages.attachments/get \n",
    "# # Method to download email attachments from gmail api\n",
    "##\n",
    "# userId dwdechallenge@gmail.com / me (authenticated user)\n",
    "# messageId \n",
    "# id  2022-09-15-23-04-08-EDT-Historical-Report-ENTRFacebook-2022-06-16--2022-09-16.csv \n",
    "#GET https://gmail.googleapis.com/gmail/v1/users/{userId}/messages/{messageId}/attachments/{id}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googleapiclient.discovery import build\n",
    "from google.oauth2.credentials import Credentials\n",
    "from googleapiclient.errors import HttpError\n",
    "import base64\n",
    "import os\n",
    "import time\n",
    "import sys \n",
    "\n",
    "# Import functions from source folder\n",
    "sys.path.append('../src/') \n",
    "from api_functions import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 17] File exists: '../data/downloaded'\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'service' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 34\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mattachmentId\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m body:\n\u001b[1;32m     33\u001b[0m     attachment_id \u001b[39m=\u001b[39m body[\u001b[39m'\u001b[39m\u001b[39mattachmentId\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m---> 34\u001b[0m     attachment_content\u001b[39m=\u001b[39m get_file_data(email_message[\u001b[39m'\u001b[39;49m\u001b[39mid\u001b[39;49m\u001b[39m'\u001b[39;49m],attachment_id,file_name,save_location)\n\u001b[1;32m     35\u001b[0m     \u001b[39mprint\u001b[39m(file_name)\n\u001b[1;32m     37\u001b[0m     \u001b[39m#with open(os.path.join(save_location + '/' 'Downloaded_files' + '/' + file_name),'wb') as _f:\u001b[39;00m\n",
      "File \u001b[0;32m~/SCIENCE/DW_Challenge/notebooks/../src/api_functions.py:53\u001b[0m, in \u001b[0;36mget_file_data\u001b[0;34m(message_id, attachment_id, file_name, save_location)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_file_data\u001b[39m(message_id,attachment_id,file_name,save_location):\n\u001b[0;32m---> 53\u001b[0m     response\u001b[39m=\u001b[39mservice\u001b[39m.\u001b[39musers()\u001b[39m.\u001b[39mmessages()\u001b[39m.\u001b[39mattachments()\u001b[39m.\u001b[39mget(\n\u001b[1;32m     54\u001b[0m         userId\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mme\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     55\u001b[0m         messageId\u001b[39m=\u001b[39mmessage_id,\n\u001b[1;32m     56\u001b[0m         \u001b[39mid\u001b[39m\u001b[39m=\u001b[39mattachment_id\n\u001b[1;32m     57\u001b[0m     )\u001b[39m.\u001b[39mexecute()\n\u001b[1;32m     59\u001b[0m     file_data\u001b[39m=\u001b[39mbase64\u001b[39m.\u001b[39murlsafe_b64decode(response\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mencode(\u001b[39m'\u001b[39m\u001b[39mUTF-8\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[1;32m     60\u001b[0m     \u001b[39mreturn\u001b[39;00m file_data\n",
      "\u001b[0;31mNameError\u001b[0m: name 'service' is not defined"
     ]
    }
   ],
   "source": [
    "# If modifying these scopes, delete the file token.json.\n",
    "SCOPES = ['https://www.googleapis.com/auth/gmail.readonly']\n",
    "creds = Credentials.from_authorized_user_file('../token.json', SCOPES)\n",
    "service = build('gmail', 'v1', credentials=creds)\n",
    "\n",
    "## search for emails with 'report' in the attachment name\n",
    "query_string='filename:report*csv' #find all the email with attachment in csv format containing 'report' in their name\n",
    "# other possible queries\n",
    "#query_string='report' # all email with report in subject/body text\n",
    "#query_string='has:attachment' # all email with attachments\n",
    "email_messages = search_email(service, query_string,['INBOX'])\n",
    "try:\n",
    "    path = os.path.join('../', 'data/downloaded')\n",
    "    os.mkdir(path)\n",
    "except OSError as error:\n",
    "    print(error) \n",
    "save_location='../data/downloaded'\n",
    "## download attachments for found emails \n",
    "\n",
    "for email_message in email_messages:\n",
    "    #print(email_message['id'])\n",
    "    #187ffb2fbf3c1bc5\n",
    "    messageDetail = get_message_detail(service,email_message['id'],msg_format='full',metadata_headers=['parts'])\n",
    "    #print(messageDetail)\n",
    "    messageDetailPayload = messageDetail.get('payload')\n",
    "\n",
    "    if 'parts' in messageDetailPayload:\n",
    "        for msgPayload in messageDetailPayload['parts']:\n",
    "            file_name = msgPayload['filename']\n",
    "            body=msgPayload['body']\n",
    "\n",
    "            if 'attachmentId' in body:\n",
    "                attachment_id = body['attachmentId']\n",
    "                attachment_content= get_file_data(service,email_message['id'],attachment_id,file_name,save_location)\n",
    "                print(file_name)\n",
    "                \n",
    "                #with open(os.path.join(save_location + '/' 'Downloaded_files' + '/' + file_name),'wb') as _f:\n",
    "                with open(os.path.join(save_location + '/' + file_name),'wb') as _f:\n",
    "                    _f.write(attachment_content)\n",
    "                    print(f'File {file_name} is saved at {save_location}')\n",
    "    time.sleep(0.5)\n",
    "    #break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the data from the files and upload them to the postgres database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import psycopg2\n",
    "import os\n",
    "import glob\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#downloaded report .csv files\n",
    "\n",
    "daily_repo_filepath='../data/downloaded/' # downloaded files via API\n",
    "files=glob.glob(os.path.join(daily_repo_filepath,\"*.csv\"))\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the data with pandas\n",
    "data = pd.read_csv(files[0])\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to postgres database\n",
    "#parameters\n",
    "param_dic = {\n",
    "    \"host\"      : \"localhost\",\n",
    "    \"database\"  : \"postgres\",\n",
    "    \"user\"      : \"postgres\",\n",
    "}\n",
    "\n",
    "#connect to postgres server\n",
    "def connect(params_dic):\n",
    "    \"\"\" Connect to the PostgreSQL database server \"\"\"\n",
    "    conn = None\n",
    "    try:\n",
    "        # connect to the PostgreSQL server\n",
    "        print('Connecting to the PostgreSQL database...')\n",
    "        conn = psycopg2.connect(**params_dic)\n",
    "    except (Exception, psycopg2.DatabaseError) as error:\n",
    "        print(error)\n",
    "        sys.exit(1) \n",
    "    print(\"Connection successful\")\n",
    "    return conn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# execute query\n",
    "\n",
    "def execute_query(conn, query):\n",
    "    \"\"\" Execute a single query \"\"\"\n",
    "    conn = connect(param_dic)\n",
    "    ret = 0 # Return value\n",
    "    cursor = conn.cursor()\n",
    "    try:\n",
    "        cursor.execute(query)\n",
    "        conn.commit()\n",
    "    except (Exception, psycopg2.DatabaseError) as error:\n",
    "        print(\"Error: %s\" % error)\n",
    "        conn.rollback()\n",
    "        cursor.close()\n",
    "        return 1\n",
    "\n",
    "    # If this was a select query, return the result\n",
    "    if 'select' in query.lower():\n",
    "        ret = cursor.fetchall()\n",
    "    cursor.close()\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create postgres database\n",
    "\n",
    "# column names from pandas dataframe csv file\n",
    "cols=','.join([column.replace(\" \", \"_\").replace(\"?\",\"\").lower() + str(' VARCHAR') for column in data.columns.values])\n",
    "\n",
    "#name of a new postgres database \n",
    "new_table='table1'\n",
    "\n",
    "#query=(\"DROP TABLE IF EXISTS %s;\"%(new_table))\n",
    "query=(\"CREATE TABLE IF NOT EXISTS %s(%s)\" %(new_table,(cols))) \n",
    "\n",
    "\n",
    "#conn = connect(param_dic)\n",
    "execute_query(conn, query)\n",
    "#create_postgres(data,table)\n",
    "\n",
    "#name of the postgres database to upload data\n",
    "table=new_table\n",
    "\n",
    "#upload data from csv files to postreg database\n",
    "[upload_csv_to_postgres(table,files) for file in files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2nd option create postgres database\n",
    "def create_postgres (db,new_table):\n",
    "    cols=cols=','.join([column.replace(\" \", \"_\").replace(\"?\",\"\").lower() + str(' VARCHAR') for column in data.columns.values])\n",
    "\n",
    "    conn = connect(param_dic)\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    #drop table if already exists\n",
    "    cur.execute(\"DROP TABLE IF EXISTS %s;\"%(new_table))\n",
    "    #create database\n",
    "    cur.execute(\"CREATE TABLE IF NOT EXISTS %s(%s)\" %(new_table,(cols))) \n",
    "    conn.commit()\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Upload data to the postgres database\n",
    "\n",
    "def upload_csv_to_postgres(table, csv):\n",
    "    conn = connect(param_dic)\n",
    "    cur = conn.cursor()\n",
    "    try:\n",
    "        [cur.copy_expert(\"COPY %s FROM STDIN WITH DELIMITER ',' CSV \"%(table), open(csv_file)) for csv_file in files]\n",
    "    except (Exception, psycopg2.DatabaseError) as error:\n",
    "        print(\"Error: %s\" % error)\n",
    "        conn.rollback()\n",
    "        cur.close()\n",
    "        return 1\n",
    "\n",
    "    conn.commit()\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = connect(param_dic)\n",
    "execute_query(conn, 'SELECT Page_Name User_Name FROM new_table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##SQL script to process the data and push the data to GIT\n",
    "## create tables \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Have a look at the dwh_dl_facebook_post_insights.csv. You will find there 3 rows for seperate posts. Import the csv to a database table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_insights='/Users/anya/SCIENCE/DW_Challenge/Junior_DE_Task/'\n",
    "db_insights=pd.read_csv(path_insights+'dwh_dl_facebook_post_insights.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "table='insights_draft'\n",
    "data=db_insights\n",
    "files=glob.glob(os.path.join(path_insights,\"*.csv\"))\n",
    "#conn = connect(param_dic)\n",
    "create_postgres(data,table)\n",
    "upload_csv_to_postgres(table,files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Please write a SQL script to unpack the json in column post_video_view_time_by_age_bucket_and_gender in a new table, while preserving the other columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SELECT \n",
    "   id, \n",
    "   data::json->'name' as name,\n",
    "   data::json->'author' ->> 'last_name' as author\n",
    "FROM books;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = connect(param_dic)\n",
    "\n",
    "sel_column='post_video_view_time_by_age_bucket_and_gender'\n",
    "table='insights_draft'\n",
    "execute_query(conn,'SELECT %s FROM %s'%(sel_column,table))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execute_query(conn,'SELECT %s FROM %s'%(sel_column,table))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execute_query(conn, \"SELECT %s  FROM %s, data::jsons->'U.18-24' as U.18-24, data::jsons->'U.25-34' as U.25-34\"%(sel_column,table))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execute_query(conn,'SELECT %s FROM %s, json_each(%s)'%(sel_column,table,sel_column))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sel_column)\n",
    "conn = connect(param_dic)\n",
    "execute_query(conn, \"SELECT %s json_data -> '%s' %s, FROM %s;\" %(sel_column,sel_column,'foo',table))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = psycopg2.connect(\"host=localhost dbname=postgres user=postgres\")\n",
    "cur = conn.cursor()\n",
    "cur.execute(\"\"\"\n",
    "    SELECT json_data \n",
    "        post_video_view_time_by_age_bucket_and_gender -> \"U.18-24\" as U1\n",
    "    FROM insights_draft;\n",
    "    \"\"\")\n",
    "conn.commit()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = psycopg2.connect(\"host=localhost dbname=postgres user=postgres\")\n",
    "cur = conn.cursor()\n",
    "cur.execute(\"\"\"\n",
    "    SELECT (post_video_view_time_by_age_bucket_and_gender) from insights_draft;\n",
    "    \"\"\")\n",
    "conn.commit()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execute_query(conn,'select jsonb_object_keys(%s) from %s;'%(sel_column,table))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sel_column)\n",
    "conn = connect(param_dic)\n",
    "execute_query(conn, \"SELECT %s -> 'U.18-24' AS U1 FROM %s\" %(sel_column,table))\n",
    "\n",
    "#SELECT post_video_view_time_by_age_bucket_and_gender FROM insights_draft;\n",
    "\n",
    "#SELECT post_video_view_time_by_age_bucket_and_gender json_data ->> 'U.18-24' AS U1 FROM insights_draft; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execute_query(conn,\"select \n",
    "    json_data #> \n",
    "       '{pizzas,1,additionalToppings}'  \n",
    "        as additional_toppings_2nd_pizza\n",
    "from test;\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write a script for the sums per post for each gender. Additionally create the sum for age 18-34 of all genders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SELECT SUM(Quantity) FROM OrderDetails;\n",
    "# add where statement WHERE Country='Mexico' WHERE Country='Mexico'\n",
    "# SELECT * FROM Products WHERE Price BETWEEN 50 AND 60;"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvDE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
